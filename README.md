<a id="top"></a>

# DataPulse (æ•°æ®è„‰æ) Intelligence Hub

> è‹±æ–‡åï¼šDataPulseï½œä¸­æ–‡åï¼šæ•°æ®è„‰æ  
> ç‚¹å‡»åˆ‡æ¢åˆ°å¯¹åº”è¯­è¨€ç‰ˆæœ¬ï¼š  
> <a href="./README_CN.md">ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆ</a> | <a href="./README_EN.md">ğŸ‡ºğŸ‡¸ English version</a>

<details open>
<summary><b>ğŸ‡¨ğŸ‡³ ä¸­æ–‡</b></summary>

## æ•°æ®è„‰æï¼ˆDataPulseï¼‰æ ¸å¿ƒç›®æ ‡

åœ¨ç»Ÿä¸€å…¥å£ä¸‹å®Œæˆè·¨å¹³å°å†…å®¹é‡‡é›†ã€ç»“æ„åŒ–æŠ½å–ã€ç½®ä¿¡åº¦è¯„ä¼°ä¸æŒä¹…åŒ–è¾“å‡ºï¼Œæ”¯æŒåç»­æ¥å…¥ MCP / Skill / Agent / Bot çš„æ ‡å‡†åŒ–ç»“æœæµã€‚

## å½“å‰å®ç°èƒ½åŠ›ï¼ˆæŒ‰ä»£ç å®é™…ï¼‰

- è·¯ç”±ä¸é‡‡é›†å™¨ï¼š`twitter/x`, `reddit`, `youtube`, `bilibili`, `telegram`, `wechat`, `xiaohongshu`, `rss`, `arxiv`, `hackernews`, `trending`, `generic web`, `jina`
- é‡‡é›†ç­–ç•¥ï¼š
  - Twitterï¼šFxTwitter + Nitter å…œåº•
  - Redditï¼šå…¬å¼€ JSON API
  - YouTubeï¼šå­—å¹•ä¼˜å…ˆï¼Œç¼ºå¤±æ—¶å¯å›é€€åˆ°éŸ³é¢‘è½¬å†™ï¼ˆ`GROQ_API_KEY`ï¼‰
  - Bilibiliï¼šå®˜æ–¹ API + äº¤äº’æ•°æ®ï¼ˆæ’­æ”¾/ç‚¹èµ/æŠ•å¸/æ”¶è—/å¼¹å¹•/è½¬å‘ï¼‰
  - Telegramï¼šTelethonï¼ˆ`TG_API_ID`/`TG_API_HASH`ï¼‰ï¼Œæ”¯æŒ `DATAPULSE_TG_*` å¯é…ç½®é™åˆ¶
  - WeChat / Xiaohongshuï¼šJina å…œåº• + é‡è¯•ï¼Œæ”¯æŒ Playwright Session å›é€€ï¼ŒXHS è‡ªåŠ¨æå–äº’åŠ¨æŒ‡æ ‡ï¼ˆèµ/è¯„è®º/æ”¶è—/åˆ†äº«ï¼‰ï¼ŒSession TTL ç¼“å­˜
  - RSSï¼šå¤šæ¡ç›® Feed è§£æï¼ˆæœ€å¤š 5 æ¡ï¼‰ï¼Œè‡ªåŠ¨è¯†åˆ« feed ç±»å‹
  - arXivï¼šAtom API è§£æè®ºæ–‡å…ƒæ•°æ®ï¼ˆæ ‡é¢˜/ä½œè€…/æ‘˜è¦/åˆ†ç±»/PDF é“¾æ¥ï¼‰
  - Hacker Newsï¼šFirebase API åŠ¨æ€æŠ“å–ï¼Œengagement è‡ªåŠ¨æ ‡è®°
  - Trendingï¼štrends24.in å…¨çƒ 400+ åœ°åŒº X/Twitter çƒ­æœè¶‹åŠ¿æŠ“å–ï¼Œæ”¯æŒåœ°åŒºåˆ«åï¼ˆus/uk/jp ç­‰ 30+ï¼‰ï¼Œå°æ—¶çº§å¿«ç…§ï¼ŒTweet é‡çº§è§£æ
  - é€šç”¨ç½‘é¡µï¼šTrafilatura/BeautifulSoupï¼Œå¤±è´¥æ—¶å¯å›é€€ Firecrawlï¼ˆ`FIRECRAWL_API_KEY`ï¼‰æˆ– Jina Reader
  - Jina å¢å¼ºè¯»å–ï¼šCSS é€‰æ‹©å™¨å®šå‘æŠ“å–ã€ç­‰å¾…å…ƒç´ åŠ è½½ã€Cookie é€ä¼ ã€ä»£ç†ã€AI å›¾ç‰‡æè¿°ã€ç¼“å­˜æ§åˆ¶
  - Web æœç´¢ï¼šé€šè¿‡ Jina Search API (`s.jina.ai`) æœç´¢å…¨ç½‘ï¼Œè‡ªåŠ¨æå–å¹¶è¯„åˆ†ï¼Œæ”¯æŒå¹³å°é™å®šæœç´¢ï¼ˆ`--platform`ï¼‰
- åŒå±‚è¾“å‡ºï¼š
  - ç»“æ„åŒ– JSONï¼ˆç»Ÿä¸€ `DataPulseItem`ï¼‰
  - å¯é€‰ Markdown è®°å¿†å†™å…¥ï¼ˆ`datapulse-inbox.md` / è‡ªå®šä¹‰è·¯å¾„ï¼‰
- å¤šç»´è¯„åˆ†ï¼šå››ç»´åº¦åŠ æƒï¼ˆç½®ä¿¡åº¦/æ¥æºæƒå¨/è·¨æºäº’è¯/æ—¶æ•ˆæ€§ï¼‰ï¼Œè¾“å‡º 0-100 ç»¼åˆåˆ† + 0.01~0.99 ç½®ä¿¡åˆ†
- Digest æ„å»ºï¼šè‡ªåŠ¨ç”ŸæˆåŒ…å« primary/secondary æ•…äº‹çš„æ‘˜è¦ä¿¡å°ï¼Œæ”¯æŒæŒ‡çº¹å»é‡ä¸å¤šæ ·æ€§é€‰æ‹©
- ç¨³å¥æ€§ï¼š
  - ç»Ÿä¸€é”™è¯¯å¤„ç†ï¼Œå¼‚å¸¸çª„åŒ–ï¼ˆç²¾ç¡®æ•è· `RequestException`/`TimeoutError` ç­‰ï¼‰
  - `retry_with_backoff` é‡è¯•è£…é¥°å™¨ + `CircuitBreaker` ç†”æ–­å™¨
  - å†…å­˜çº§ TTL ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œæ— å¤–éƒ¨ä¾èµ–ï¼‰
  - å¹¶å‘æ‰¹é‡æ‰§è¡Œï¼ˆ`read_batch`ï¼‰ï¼Œè‡ªåŠ¨ URL å»é‡
  - å»é‡ä¸è¿‡æœŸè£å‰ªï¼ˆé»˜è®¤æœ€å¤š 500 æ¡ã€30 å¤©ï¼‰
- å¯è§‚æµ‹æ€§ï¼š
  - ç»“æ„åŒ–æ—¥å¿—ï¼ˆ`DATAPULSE_LOG_LEVEL` ç¯å¢ƒå˜é‡æ§åˆ¶çº§åˆ«ï¼‰
- æµ‹è¯•åŸºå»ºï¼š
  - 420+ å•å…ƒæµ‹è¯•ï¼Œè¦†ç›– 21 ä¸ªæµ‹è¯•æ¨¡å—
  - GitHub Actions CIï¼ˆPython 3.10 / 3.11 / 3.12 çŸ©é˜µï¼‰

## å®‰è£…

```bash
pip install -e .
pip install -e ".[all]"   # å¯ç”¨å…¨éƒ¨å¯é€‰èƒ½åŠ›
```

å¯é€‰åˆ†ç»„ï¼š

- `.[trafilatura]`ã€`.[youtube]`ã€`.[telegram]`ã€`.[browser]`ã€`.[mcp]`ã€`.[notebooklm]`

## å¼€å‘ç¯å¢ƒ

```bash
pip install -e ".[dev]"
pip install pre-commit && pre-commit install
```

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ã€ŒDataPulse Non-Commercial License v1.0ã€ã€‚
ä»…å…è®¸éå•†ä¸šç”¨é€”ï¼ˆæ•™å­¦ã€ç§‘ç ”ã€ä¸ªäººç ”ç©¶ã€å†…éƒ¨è¯„ä¼°ç­‰ï¼‰å†…å…è´¹ä½¿ç”¨ï¼›å•†ä¸šä½¿ç”¨è¯·è”ç³»ä½œè€…è·å–å•†ä¸šæˆæƒã€‚

è¯¦ç»†æ¡æ¬¾è¯·æŸ¥çœ‹ä»“åº“æ ¹ç›®å½• `LICENSE` æ–‡ä»¶ã€‚

## å¿«é€Ÿå¼€å§‹

### 1) CLI åŸºç¡€ç”¨æ³•

```bash
# è§£æå•æ¡ URL
datapulse https://x.com/xxxx/status/123

# æ‰¹é‡è§£æï¼ˆå¹¶å‘ï¼‰
datapulse --batch https://x.com/... https://www.reddit.com/... --min-confidence 0.45

# åˆ—å‡ºå†…å­˜ï¼ˆæŒ‰ç½®ä¿¡é™åºï¼‰
datapulse --list --limit 10 --min-confidence 0.30

# ç™»å½•çŠ¶æ€é‡‡é›†ï¼ˆç”¨äºç™»å½•ååœºæ™¯ï¼‰
datapulse --login xhs
datapulse --login wechat

# æ¸…ç©ºå†…å­˜
datapulse --clear

# Web æœç´¢
datapulse --search "LLM inference optimization"
datapulse --search "Python 3.13" --site python.org --site peps.python.org
datapulse --search "RAG best practices" --search-limit 10 --min-confidence 0.7

# å¹³å°é™å®šæœç´¢
datapulse --search "æŠ¤è‚¤" --platform xhs --search-limit 3

# çƒ­æœè¶‹åŠ¿
datapulse --trending              # å…¨çƒçƒ­æœ
datapulse --trending us           # ç¾å›½çƒ­æœ
datapulse --trending jp --trending-limit 10  # æ—¥æœ¬ Top 10
datapulse --trending uk --trending-store     # è‹±å›½çƒ­æœï¼Œå­˜å…¥ inbox

# å®šå‘æŠ“å–
datapulse https://example.com --target-selector ".article-body" --no-cache
```

### 2) Smoke æµ‹è¯•å‘½ä»¤

```bash
# ä»…åˆ—å‡ºéœ€è¦é…ç½®çš„ç¯å¢ƒå˜é‡
datapulse-smoke --list

# æŒ‰å¹³å°å›å½’
datapulse-smoke --platforms xhs wechat --require-all

# è¿è¡Œå…¨éƒ¨åœºæ™¯ï¼ˆå¯è®¾ç½®ç½®ä¿¡é˜ˆå€¼ï¼‰
datapulse-smoke --min-confidence 0.45
```

`datapulse-smoke` çš„ç¯å¢ƒå˜é‡è¾“å…¥é¡¹ï¼š

- `DATAPULSE_SMOKE_TWITTER_URL`
- `DATAPULSE_SMOKE_REDDIT_URL`
- `DATAPULSE_SMOKE_YOUTUBE_URL`
- `DATAPULSE_SMOKE_BILIBILI_URL`
- `DATAPULSE_SMOKE_TELEGRAM_URL`
- `DATAPULSE_SMOKE_RSS_URL`
- `DATAPULSE_SMOKE_WECHAT_URL`
- `DATAPULSE_SMOKE_XHS_URL`

## MCP / Skill / Agent ä½¿ç”¨

- MCP æœåŠ¡ç«¯ï¼ˆéœ€å®‰è£… `.[mcp]`ï¼‰ï¼š

```bash
python -m datapulse.mcp_server
```

æš´éœ² 23 ä¸ªå·¥å…·ï¼š

**é‡‡é›†ä¸è¯»å–ï¼š**
- `read_url(url, min_confidence=0.0)` â€” è§£æå•æ¡ URL
- `read_batch(urls, min_confidence=0.0)` â€” æ‰¹é‡è§£æ URL
- `read_url_advanced(url, target_selector, wait_for_selector, no_cache, with_alt, min_confidence)` â€” CSS å®šå‘æŠ“å–
- `search_web(query, sites, platform, limit, fetch_content, min_confidence)` â€” Web æœç´¢
- `trending(location, top_n, store)` â€” X/Twitter çƒ­æœè¶‹åŠ¿

**å†…å­˜ä¸çŠ¶æ€ï¼š**
- `query_inbox(limit, min_confidence)` â€” æŸ¥è¯¢æ”¶ä»¶ç®±
- `mark_processed(item_id, processed)` â€” æ ‡è®°å·²å¤„ç†
- `query_unprocessed(limit, min_confidence)` â€” æŸ¥è¯¢æœªå¤„ç†æ¡ç›®

**ä¿¡æºç®¡ç†ï¼š**
- `list_sources(include_inactive, public_only)` â€” åˆ—å‡ºä¿¡æºç›®å½•
- `list_packs(public_only)` â€” åˆ—å‡ºä¿¡æºåŒ…
- `resolve_source(url)` â€” URL ä¿¡æºè¯†åˆ«
- `list_subscriptions(profile)` â€” åˆ—å‡ºè®¢é˜…
- `source_subscribe(profile, source_id)` â€” è®¢é˜…ä¿¡æº
- `source_unsubscribe(profile, source_id)` â€” å–æ¶ˆè®¢é˜…
- `install_pack(profile, slug)` â€” å®‰è£…ä¿¡æºåŒ…

**Feed ä¸ Digestï¼š**
- `query_feed(profile, source_ids, limit, min_confidence, since)` â€” æŸ¥è¯¢ Feed
- `build_json_feed(profile, source_ids, limit, min_confidence, since)` â€” JSON Feed
- `build_rss_feed(profile, source_ids, limit, min_confidence, since)` â€” RSS Feed
- `build_atom_feed(profile, source_ids, limit, min_confidence, since)` â€” Atom 1.0 Feed
- `build_digest(profile, source_ids, top_n, secondary_n, min_confidence, since)` â€” ç²¾é€‰æ‘˜è¦

**å·¥å…·ï¼š**
- `detect_platform(url)` â€” å¹³å°æ£€æµ‹
- `health()` â€” å¥åº·æ£€æŸ¥

- Skill å…¥å£ï¼ˆå¯ä¾› OpenClaw Skill æ¥å…¥ï¼‰ï¼š

```python
from datapulse_skill import run
run("è¯·å¤„ç†è¿™äº›ä¿¡æ¯: https://x.com/... æˆ– https://reddit.com/...")
```

- Agent è°ƒç”¨ï¼ˆå¯è¢«ä¸Šå±‚ Bot æ¡†æ¶å°è£…ï¼‰ï¼š

```python
from datapulse.agent import DataPulseAgent

agent = DataPulseAgent(min_confidence=0.25)
result = await agent.handle("https://x.com/... and https://www.reddit.com/...")
```

## é…ç½®ä¸ç¯å¢ƒå˜é‡

- `INBOX_FILE`
- `DATAPULSE_MEMORY_DIR`
- `DATAPULSE_KEEP_DAYS`ï¼ˆé»˜è®¤ 30ï¼‰
- `DATAPULSE_MAX_INBOX`ï¼ˆé»˜è®¤ 500ï¼‰
- `OUTPUT_DIR`
- `DATAPULSE_MARKDOWN_PATH`
- `OBSIDIAN_VAULT`
- `DATAPULSE_SESSION_DIR`ï¼ˆé»˜è®¤ `~/.datapulse/sessions`ï¼‰
- `TG_API_ID` / `TG_API_HASH`
- `NITTER_INSTANCES`
- `FXTWITTER_API_URL`
- `FIRECRAWL_API_KEY`
- `GROQ_API_KEY`
- `DATAPULSE_LOG_LEVEL`ï¼ˆé»˜è®¤ WARNINGï¼‰
- `DATAPULSE_TG_MAX_MESSAGES`ï¼ˆé»˜è®¤ 20ï¼‰
- `DATAPULSE_TG_MAX_CHARS`ï¼ˆé»˜è®¤ 800ï¼‰
- `DATAPULSE_TG_CUTOFF_HOURS`ï¼ˆé»˜è®¤ 24ï¼‰
- `DATAPULSE_SMOKE_*`
- `DATAPULSE_BATCH_CONCURRENCY`ï¼ˆé»˜è®¤ 5ï¼‰
- `DATAPULSE_MIN_CONFIDENCE`
- `DATAPULSE_SESSION_TTL_HOURS`ï¼ˆé»˜è®¤ 12ï¼ŒSession ç¼“å­˜ TTL å°æ—¶æ•°ï¼‰
- `JINA_API_KEY`ï¼ˆJina å¢å¼ºè¯»å– + Web æœç´¢ API Keyï¼‰

## ä½¿ç”¨å»ºè®®ï¼ˆopenclaw-bot åœºæ™¯ï¼‰

- MCP ä¾§å»ºè®®ç”¨ `read_url/read_batch` äº§ç”Ÿç»“æ„åŒ– JSONï¼ŒæŒ‰ `source_type/confidence/tags` åšäºŒæ¬¡è·¯ç”±ã€‚
- Skill ä¾§å»ºè®®ç”¨ `DataPulseItem` çš„ `to_dict()` ç»“æœç›´æ¥å…¥é˜Ÿï¼Œé¿å…å¤å†™è§£æé€»è¾‘ã€‚
- Agent ä¾§å»ºè®®åœ¨å¯¹è¯ä¸­å…ˆæŠ½ URLï¼Œå†è°ƒç”¨ `agent.handle` åšæ‰¹é‡æ‰¹æ³¨ä¸é™ç½®ä¿¡ã€‚
- ç»Ÿä¸€è®°å¿†è·¯å¾„å»ºè®®é…ç½®ç‹¬ç«‹ç›®å½•ï¼Œä¾¿äº Bot å¤šå®ä¾‹å…±äº«å·²é‡‡é›†ç»“æœã€‚

## å®‰å…¨ä¸è¾¹ç•Œ

- å½“å‰å®ç°å¯¹ URL åšåŸºç¡€å¯è¾¾æ€§ä¸åŸŸåç­–ç•¥æ ¡éªŒï¼ˆæ‹¦æˆªå†…ç½‘/æœ¬åœ°åœ°å€ä¸éå…¬ç½‘è§£æï¼‰ã€‚
- `read_batch` é»˜è®¤è·³è¿‡å•æ¡å¤±è´¥ï¼Œ`return_all=False` å¯æ”¹ä¸ºé‡é”™å³æŠ›ã€‚

## è¯´æ˜

- ä»“åº“æ–‡æ¡£ä¸åŒ…å«æœ¬åœ°æµ‹è¯•ç¯å¢ƒä¸æ¨¡å‹ç«¯ç‚¹æ˜æ–‡ä¿¡æ¯ã€‚
- æ•æ„Ÿé…ç½®éœ€æ”¾å…¥ä½ çš„ç§æœ‰è¿è¡Œæ—¶ç¯å¢ƒï¼Œé€šè¿‡å®‰å…¨å‡­è¯ç®¡ç†æ–¹å¼æ³¨å…¥ã€‚

## OpenClaw å¯¹æ¥èµ„äº§ï¼ˆå»ºè®®ï¼‰

- å·¥å…·å¥‘çº¦æ¨¡æ¿ï¼š`docs/contracts/openclaw_datapulse_tool_contract.json`
- å¿«é€ŸéªŒæ”¶è„šæœ¬ï¼š`scripts/datapulse_local_smoke.sh`ã€`scripts/datapulse_remote_openclaw_smoke.sh`
- å‘å¸ƒæ¸…å•ï¼š`docs/release_checklist.md`

```bash
chmod +x scripts/datapulse_local_smoke.sh scripts/datapulse_remote_openclaw_smoke.sh
export URL_1="https://x.com/xxxx/status/123"
export URL_BATCH="https://x.com/... https://www.reddit.com/..."
bash scripts/datapulse_local_smoke.sh
# è¿œç«¯ï¼ˆéœ€å…ˆé…ç½® VPS/M4 è¿æ¥å˜é‡ï¼‰
bash scripts/datapulse_remote_openclaw_smoke.sh
```

## å‘å¸ƒä¸ç‰ˆæœ¬ç»‘å®šï¼ˆReleaseï¼‰

- å‘å¸ƒèµ„äº§ï¼š
  - `python -m build --sdist --wheel .`
  - é™„åŠ  `dist/*.whl` ä¸ `dist/*.tar.gz`
- è‡ªåŠ¨åŒ–ï¼š
  - `./scripts/release_publish.sh --tag vX.Y.Z`
  - æ¨é€ tag åç”± `.github/workflows/release.yml` è‡ªåŠ¨é™„åŠ èµ„äº§åˆ° GitHub Release

[ğŸ”¼ å›åˆ°é¡¶éƒ¨](#top) | [ğŸ‡¨ğŸ‡³ ä¸­æ–‡è¯¦æƒ…é¡µ](./README_CN.md) | [ğŸ‡ºğŸ‡¸ English details](./README_EN.md)

</details>

---

<a id="top"></a>

<details>
<summary><b>ğŸ‡ºğŸ‡¸ English</b></summary>

## Core goal

Build a single intake path for URL content extraction, confidence scoring, and memory output,
with structured results that can feed MCP, Assistant Skill, Agent, or Bot workflows.

## Implemented capabilities

- Router and collectors: `twitter/x`, `reddit`, `youtube`, `bilibili`, `telegram`, `wechat`, `xiaohongshu`, `rss`, `arxiv`, `hackernews`, `trending`, `generic web`, `jina`
- Collector strategy:
  - Twitter: FxTwitter primary + Nitter fallback
  - Reddit: public JSON API
  - YouTube: transcript first, optional audio transcription fallback (`GROQ_API_KEY`)
  - Bilibili: official API + interaction stats (views/likes/coins/favorites/danmaku/shares)
  - Telegram: Telethon (`TG_API_ID`/`TG_API_HASH`), configurable via `DATAPULSE_TG_*` env vars
  - WeChat / Xiaohongshu: Jina fallback with retry, optional Playwright session fallback, XHS auto-extracts engagement metrics (likes/comments/favorites/shares), session TTL cache
  - RSS: multi-entry feed parsing (up to 5 entries), auto feed type detection
  - arXiv: Atom API parsing for paper metadata (title/authors/abstract/categories/PDF link)
  - Hacker News: Firebase API with dynamic engagement flags
  - Trending: trends24.in scraper for X/Twitter trending topics across 400+ global locations, 30+ location aliases (us/uk/jp etc.), hourly snapshots, tweet volume parsing
  - Generic web: Trafilatura / BeautifulSoup, optional Firecrawl fallback (`FIRECRAWL_API_KEY`) or Jina Reader
  - Jina enhanced reading: CSS selector targeting, wait-for-element, cookie passthrough, proxy, AI image descriptions, cache control
  - Web search: search the web via Jina Search API (`s.jina.ai`), auto-extract and score results, platform-scoped search (`--platform`)
- Output:
  - Structured JSON (`DataPulseItem`)
  - Optional Markdown output (`datapulse-inbox.md` / custom path)
- Multi-dimensional scoring: 4-axis weighted (confidence/authority/corroboration/recency), 0-100 composite + [0.01, 0.99] confidence
- Digest builder: curated primary/secondary stories with fingerprint dedup and diversity selection
- Resilience:
  - unified parse result handling with narrowed exceptions
  - `retry_with_backoff` decorator + `CircuitBreaker` for fault tolerance
  - in-memory TTL cache (thread-safe, zero external deps)
  - concurrent batch read (`read_batch`) with auto URL dedup
  - dedupe and prune by max items / retention days
- Observability:
  - structured logging (`DATAPULSE_LOG_LEVEL` env var)
- Testing:
  - 420+ tests across 21 test modules
  - GitHub Actions CI (Python 3.10/3.11/3.12 matrix)

## Install

```bash
pip install -e .
pip install -e ".[all]"   # enable all optional capabilities
```

Optional groups:

- `.[trafilatura]`, `.[youtube]`, `.[telegram]`, `.[browser]`, `.[mcp]`, `.[notebooklm]`

## Development

```bash
pip install -e ".[dev]"
pip install pre-commit && pre-commit install
```

## License

This project is released under the **DataPulse Non-Commercial License v1.0**.
It is free for non-commercial use (e.g., education, research, personal/POC evaluation).
Commercial usage requires a separate license from the author.

See the root `LICENSE` file for full terms.

## Quick start

### 1) CLI basics

```bash
# read one URL
datapulse https://x.com/xxxx/status/123

# batch read
datapulse --batch https://x.com/... https://www.reddit.com/... --min-confidence 0.45

# list memory in confidence order
datapulse --list --limit 10 --min-confidence 0.30

# store login sessions when needed
datapulse --login xhs
datapulse --login wechat

# clear memory
datapulse --clear

# web search
datapulse --search "LLM inference optimization"
datapulse --search "Python 3.13" --site python.org --site peps.python.org

# trending topics
datapulse --trending              # worldwide
datapulse --trending us           # United States
datapulse --trending jp --trending-limit 10  # Japan top 10
datapulse --trending uk --trending-store     # UK, save to inbox

# targeted extraction
datapulse https://example.com --target-selector ".article-body" --no-cache
```

### 2) Smoke check

```bash
# show required environment keys for smoke tests
datapulse-smoke --list

# run selected platforms
datapulse-smoke --platforms xhs wechat --require-all

# run all configured scenarios
datapulse-smoke --min-confidence 0.45
```

Smoke env vars:

- `DATAPULSE_SMOKE_TWITTER_URL`
- `DATAPULSE_SMOKE_REDDIT_URL`
- `DATAPULSE_SMOKE_YOUTUBE_URL`
- `DATAPULSE_SMOKE_BILIBILI_URL`
- `DATAPULSE_SMOKE_TELEGRAM_URL`
- `DATAPULSE_SMOKE_RSS_URL`
- `DATAPULSE_SMOKE_WECHAT_URL`
- `DATAPULSE_SMOKE_XHS_URL`

## MCP / Skill / Agent

- MCP server (install with `.[mcp]`):

```bash
python -m datapulse.mcp_server
```

23 exposed tools:

**Intake & reading:**
- `read_url(url, min_confidence)` â€” parse a single URL
- `read_batch(urls, min_confidence)` â€” batch parse URLs
- `read_url_advanced(url, target_selector, wait_for_selector, no_cache, with_alt, min_confidence)` â€” CSS-targeted extraction
- `search_web(query, sites, platform, limit, fetch_content, min_confidence)` â€” web search
- `trending(location, top_n, store)` â€” X/Twitter trending topics

**Memory & state:**
- `query_inbox(limit, min_confidence)` â€” query inbox
- `mark_processed(item_id, processed)` â€” mark as processed
- `query_unprocessed(limit, min_confidence)` â€” query unprocessed items

**Source management:**
- `list_sources(include_inactive, public_only)` â€” list source catalog
- `list_packs(public_only)` â€” list source packs
- `resolve_source(url)` â€” resolve URL to source
- `list_subscriptions(profile)` â€” list subscriptions
- `source_subscribe(profile, source_id)` â€” subscribe to source
- `source_unsubscribe(profile, source_id)` â€” unsubscribe
- `install_pack(profile, slug)` â€” install source pack

**Feed & digest:**
- `query_feed(profile, source_ids, limit, min_confidence, since)` â€” query feed
- `build_json_feed(profile, source_ids, limit, min_confidence, since)` â€” JSON Feed
- `build_rss_feed(profile, source_ids, limit, min_confidence, since)` â€” RSS Feed
- `build_atom_feed(profile, source_ids, limit, min_confidence, since)` â€” Atom 1.0 Feed
- `build_digest(profile, source_ids, top_n, secondary_n, min_confidence, since)` â€” curated digest

**Utilities:**
- `detect_platform(url)` â€” platform detection
- `health()` â€” health check

- Skill entry (for OpenClaw/assistant adapters):

```python
from datapulse_skill import run
run("Please process: https://x.com/... and https://www.reddit.com/...")
```

- Agent usage:

```python
from datapulse.agent import DataPulseAgent

agent = DataPulseAgent(min_confidence=0.25)
result = await agent.handle("https://x.com/... and https://www.reddit.com/...")
```

## Config and env vars

- `INBOX_FILE`
- `DATAPULSE_MEMORY_DIR`
- `DATAPULSE_KEEP_DAYS` (default 30)
- `DATAPULSE_MAX_INBOX` (default 500)
- `OUTPUT_DIR`
- `DATAPULSE_MARKDOWN_PATH`
- `OBSIDIAN_VAULT`
- `DATAPULSE_SESSION_DIR` (default `~/.datapulse/sessions`)
- `TG_API_ID` / `TG_API_HASH`
- `NITTER_INSTANCES`
- `FXTWITTER_API_URL`
- `FIRECRAWL_API_KEY`
- `GROQ_API_KEY`
- `DATAPULSE_LOG_LEVEL` (default WARNING)
- `DATAPULSE_TG_MAX_MESSAGES` (default 20)
- `DATAPULSE_TG_MAX_CHARS` (default 800)
- `DATAPULSE_TG_CUTOFF_HOURS` (default 24)
- `DATAPULSE_SMOKE_*`
- `DATAPULSE_BATCH_CONCURRENCY` (default 5)
- `DATAPULSE_MIN_CONFIDENCE`
- `DATAPULSE_SESSION_TTL_HOURS` (default 12 â€” session cache TTL in hours)
- `JINA_API_KEY` (Jina API key for enhanced reading and web search)

## Recommended usage for bot/agent stacks

- MCP: call `read_url/read_batch` and route by `source_type + confidence` before tool selection.
- Skill: use returned summaries or raw `DataPulseItem.to_dict()` directly to avoid re-parsing.
- Agent: extract URLs from text first, then call `DataPulseAgent.handle` for batched processing.
- Keep memory path stable across nodes if your bot runtime needs shared cache.

## Security and boundaries

- URL routing applies public-network checks and blocks obvious local/private targets.
- `read_batch` skips failed entries by default; set strict failure behavior by code as needed.

## OpenClaw integration assets

- Tool contract: `docs/contracts/openclaw_datapulse_tool_contract.json`
- Quick validation scripts: `scripts/datapulse_local_smoke.sh`, `scripts/datapulse_remote_openclaw_smoke.sh`
- Release checklist: `docs/release_checklist.md`

```bash
chmod +x scripts/datapulse_local_smoke.sh scripts/datapulse_remote_openclaw_smoke.sh
export URL_1="https://x.com/xxxx/status/123"
export URL_BATCH="https://x.com/... https://www.reddit.com/..."
bash scripts/datapulse_local_smoke.sh
# remote execution requires VPS tunnel
bash scripts/datapulse_remote_openclaw_smoke.sh
```

## Release and publishing

- Build artifacts:
  - `python -m build --sdist --wheel .`
  - Upload `dist/*.whl` and `dist/*.tar.gz`
- Release automation:
  - `./scripts/release_publish.sh --tag vX.Y.Z`
  - GitHub Actions auto-publishes assets on tag push via `.github/workflows/release.yml`

## Notes

- Repository docs do not include local test environment or model endpoint plaintext.
- Keep sensitive runtime configuration outside repository and inject via private environment management.

[ğŸ”¼ Back to top](#top) | [ğŸ‡¨ğŸ‡³ ä¸­æ–‡è¯¦æƒ…é¡µ](./README_CN.md) | [ğŸ‡ºğŸ‡¸ English details](./README_EN.md)

</details>
